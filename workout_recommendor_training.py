# -*- coding: utf-8 -*-
"""workout_recommendor_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WZQm-ZwSnJP-Ngu5oE50jL0gBn6j0Wwf

# CREATING DATASET
"""

import random
import pandas as pd

N = 20000   # dataset size
data = []

# Label mapping:
# 0 = Full Body
# 1 = Upper Lower
# 2 = Push Pull Legs (PPL)
# 3 = PPL + Upper Lower Hybrid
# 4 = Bro Split
# 5 = Strength Focused Split
# 6 = Home Bodyweight Split

for _ in range(N):

    # ---------- RANDOM FEATURES ----------
    age = random.randint(16, 40)
    height = random.randint(150, 195)
    weight = random.randint(45, 110)
    bmi = round(weight / ((height/100)**2), 1)

    activity = random.randint(0, 3)       # 0 sedentary → 3 active
    stress = random.randint(1, 5)         # 1 low → 5 high
    sleep = random.randint(4, 9)          # hours
    experience = random.randint(0, 2)     # 0 beginner → 2 advanced
    days = random.randint(2, 6)
    duration = random.choice([30, 40, 45, 60, 75, 90])
    equipment = random.randint(0, 2)      # 0 home, 1 dumbbells, 2 full gym
    goal = random.randint(0, 2)           # 0 fat loss, 1 muscle gain, 2 fitness

    # ---------- TARGET WEIGHT ----------
    if goal == 0:   # fat loss
        target_weight = weight - random.randint(2, 10)
    elif goal == 1: # muscle gain
        target_weight = weight + random.randint(2, 8)
    else:           # general fitness
        target_weight = weight + random.randint(-2, 2)
    target_duration = random.randint(2, 8)

    # ---------- DETERMINISTIC LOGIC FOR SPLIT ----------
    recovery = sleep - 0.5*stress

    if equipment == 0:
        split = 6  # Home Bodyweight Split

    elif experience == 0:  # Beginner
        if days <= 3:
            split = 0  # Full Body
        else:
            split = 1  # Upper Lower

    elif experience == 1:  # Intermediate
        if days <= 4:
            split = 1  # Upper Lower
        else:
            split = 2  # PPL

    elif experience == 2:  # Advanced
        if days == 5:
            split = 3  # Hybrid
        elif days == 6 and duration >= 60:
            split = 4  # Bro Split
        else:
            split = 2  # PPL fallback

    # ---------- OVERRIDES ----------
    # Fat loss with poor recovery
    if goal == 0 and recovery < 3:
        split = 0

    # Strength-focused advanced
    if goal == 1 and experience == 2 and duration >= 75:
        split = 5

    # Optional small randomness (5%) to avoid totally deterministic
    if random.random() < 0.05:
        split = random.choice([0,1,2,3,4,5,6])

    data.append([
        age, height, weight, bmi,
        activity, stress, sleep,
        experience, days, duration,
        equipment, goal,
        target_weight, target_duration,
        split
    ])

# ---------- CREATE DATAFRAME ----------
columns = [
    "age", "height", "weight", "bmi",
    "activity_level", "stress", "sleep",
    "experience_level", "days_per_week",
    "session_duration_min", "equipment_access",
    "primary_goal", "target_weight_change_kg",
    "target_duration_months",
    "recommended_split"
]

df = pd.DataFrame(data, columns=columns)

# ---------- SAVE ----------
df.to_csv("workout_dataset_20000_fixed.csv", index=False)

print("Dataset generated: workout_dataset_20000_fixed.csv")
print(df["recommended_split"].value_counts())

"""# TRAINING MODEL"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

workout=pd.read_csv("/content/workout_dataset_20000_fixed.csv")
feat=workout.iloc[:,:-1]
dep=workout.iloc[:,-1]

from sklearn.model_selection import train_test_split
feat_tr,feat_test,dep_tr,dep_test=train_test_split(feat,dep,train_size=0.8,random_state=0)

from sklearn.preprocessing import StandardScaler
sc=StandardScaler()
feat_tr.iloc[:,[0,1,2,3,6,8,9,12,13]]=sc.fit_transform(feat_tr.iloc[:,[0,1,2,3,6,8,9,12,13]])
feat_test.iloc[:,[0,1,2,3,6,8,9,12,13]]=sc.transform(feat_test.iloc[:,[0,1,2,3,6,8,9,12,13]])
print(feat_tr)

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(sparse_output=False,handle_unknown='ignore'),[4,5,7,10,11])],remainder='passthrough')
feat_tr=ct.fit_transform(feat_tr)
feat_test=ct.transform(feat_test)
encoded_cols = ct.get_feature_names_out()
hot_encoded = pd.DataFrame(feat_tr, columns=encoded_cols)
print(hot_encoded)
print(encoded_cols)

feat_tr

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
feat_train_bal, dep_train_bal = smote.fit_resample(feat_tr, dep_tr)

print(pd.Series(dep_train_bal).value_counts())

from xgboost import XGBClassifier
xg_class=XGBClassifier()
xg_class.fit(feat_train_bal,dep_train_bal)

from sklearn.metrics import confusion_matrix,accuracy_score
y_pred = xg_class.predict(feat_test)
cm = confusion_matrix(dep_test, y_pred)
print(cm)
from sklearn.metrics import classification_report
print(classification_report(dep_test, y_pred))
from sklearn.model_selection import cross_val_score
accuracies = cross_val_score(estimator = xg_class, X = feat_train_bal, y = dep_train_bal, cv = 10)
print("Accuracy: {:.2f} %".format(accuracies.mean()*100))
print("Standard Deviation: {:.2f} %".format(accuracies.std()*100))

workout["recommended_split"].value_counts(normalize=True)

import joblib

joblib.dump(xg_class, "workout_model.pkl")
joblib.dump(ct, "workout_column_transformer.pkl")
joblib.dump(sc, "workout_scaler.pkl")
joblib.dump(
    ct.get_feature_names_out(),
    "workout_feature_names.pkl")

xg_class.feature_importances_

